{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"GNN.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"authorship_tag":"ABX9TyMa29XfgaZDbZiwzTMLtH7K"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Imports"],"metadata":{"id":"wPkKaqN7fh6F"}},{"cell_type":"code","source":["import dgl\n","from dgl.data import DGLDataset\n","import os\n","import pickle\n","import numpy as np\n","import random\n","import networkx as nx \n","from scipy.spatial.distance import cosine\n","from operator import itemgetter\n","from collections import OrderedDict\n","from tabulate import tabulate\n","from termcolor import colored\n","import logging\n","import copy\n","\n","import matplotlib.pyplot as plt\n","import tqdm\n","from scipy.stats import norm\n","from sklearn import metrics\n","from sklearn.metrics.pairwise import cosine_similarity\n","import torch\n","\n","import dgl.function as fn\n","import dgl.nn as dglnn\n","import torch.nn as nn\n","import torch.nn.functional as F"],"metadata":{"id":"CY9lfs6icSj_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Utils"],"metadata":{"id":"ULryavyjwWIl"}},{"cell_type":"code","source":["def print_csv_format(results):\n","    \"\"\"\n","    Print main metrics in a format similar to Detectron2,\n","    so that they are easy to copypaste into a spreadsheet.\n","    Args:\n","        results (OrderedDict): {metric -> score}\n","    \"\"\"\n","    # unordered results cannot be properly printed\n","\n","    logger = logging.getLogger(__name__)\n","\n","    dataset_name = results.pop('dataset')\n","    metrics = [\"Dataset\"] + [k for k in results]\n","    csv_results = [(dataset_name, *list(results.values()))]\n","\n","    # tabulate it\n","    table = tabulate(\n","        csv_results,\n","        tablefmt=\"pipe\",\n","        floatfmt=\".2f\",\n","        headers=metrics,\n","        numalign=\"left\",\n","    )\n","\n","    print(\"Evaluation results in csv format: \\n\" + colored(table, \"cyan\"))"],"metadata":{"id":"6rV3SR2-64Vn","executionInfo":{"status":"ok","timestamp":1649604454119,"user_tz":-120,"elapsed":41,"user":{"displayName":"Marah Gamdou","userId":"12048726292860942069"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["def eval_metrics(dataset, indices, q_ids, g_ids, q_camids, g_camids, max_rank=10):\n","    \"\"\"Evaluation of the queries output \n","    \"\"\"\n","    num_q, num_g = indices.shape\n","\n","    if num_g < max_rank:\n","        max_rank = num_g\n","        print('Note: number of gallery samples is quite small, got {}'.format(num_g))\n","\n","    # compute cmc curve for each query\n","    all_cmc = []\n","    all_AP = []\n","    all_INP = []\n","    num_valid_q = 0  # number of valid query\n","\n","    i=0\n","    for q_idx in range(num_q):\n","        # get query pid and camid\n","        q_id = q_ids[q_idx]\n","        q_camid = q_camids[q_idx]\n","\n","        order = indices[q_idx]\n","        remove = (g_ids[order] == q_id) & (g_camids[order] == q_camid)\n","        keep = np.invert(remove)\n","\n","        # compute cmc curve\n","        matches = (g_ids[order] == q_id).astype(np.int32)\n","        raw_cmc = matches[keep]  # binary vector, positions with value 1 are correct matches\n","\n","        if not np.any(raw_cmc):\n","            # this condition is true when query identity does not appear in gallery\n","            i+=1\n","            continue\n","\n","        cmc = raw_cmc.cumsum()\n","\n","        pos_idx = np.where(raw_cmc == 1)\n","        max_pos_idx = np.max(pos_idx)\n","        inp = cmc[max_pos_idx] / (max_pos_idx + 1.0)\n","        all_INP.append(inp)\n","\n","        cmc[cmc > 1] = 1\n","\n","        all_cmc.append(cmc[:max_rank])\n","        num_valid_q += 1.\n","\n","        # compute average precision\n","        num_rel = raw_cmc.sum()\n","        tmp_cmc = raw_cmc.cumsum()\n","        tmp_cmc = [x / (i + 1.) for i, x in enumerate(tmp_cmc)]\n","        tmp_cmc = np.asarray(tmp_cmc) * raw_cmc\n","        AP = tmp_cmc.sum() / num_rel\n","        all_AP.append(AP)\n","\n","    print('number of queries that do not exist in the gallery :', i)\n","    assert num_valid_q > 0, 'Error: all query identities do not appear in gallery'\n","\n","    all_cmc = np.asarray(all_cmc).astype(np.float32)\n","    all_cmc = all_cmc.sum(0) / num_valid_q\n","\n","    results = OrderedDict()\n","    results['dataset'] = dataset\n","    mAP = np.mean(all_AP)\n","    mINP = np.mean(all_INP)\n","    for r in [1, 5, 10]:\n","        results['Rank-{}'.format(r)] = all_cmc[r - 1] * 100\n","    results['mAP'] = mAP * 100\n","    results['mINP'] = mINP * 100\n","    results[\"metric\"] = (mAP + all_cmc[0]) / 2 * 100\n","\n","    print_csv_format(results)\n","    return all_cmc, all_AP, all_INP"],"metadata":{"id":"9Sv2lzIb6z-W","executionInfo":{"status":"ok","timestamp":1649604454120,"user_tz":-120,"elapsed":38,"user":{"displayName":"Marah Gamdou","userId":"12048726292860942069"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["def compute_cosine_similarity(features, others):\n","    \"\"\"Computes cosine similarity.\n","    Args:\n","        features (torch.Tensor): 2-D feature matrix.\n","        others (torch.Tensor): 2-D feature matrix.\n","    Returns:\n","        torch.Tensor: similarity matrix.\n","    \"\"\"\n","    features = F.normalize(features, p=2, dim=1)\n","    others = F.normalize(others, p=2, dim=1)\n","\n","    return torch.mm(features, others.t())"],"metadata":{"id":"5g_spQCPluMB","executionInfo":{"status":"ok","timestamp":1649604454120,"user_tz":-120,"elapsed":36,"user":{"displayName":"Marah Gamdou","userId":"12048726292860942069"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["def compute_cosine_distance(features, others):\n","    \"\"\"Computes cosine similarity.\n","    Args:\n","        features (torch.Tensor): 2-D feature matrix.\n","        others (torch.Tensor): 2-D feature matrix.\n","    Returns:\n","        torch.Tensor: similarity matrix.\n","    \"\"\"\n","    features = F.normalize(features, p=2, dim=1)\n","    others = F.normalize(others, p=2, dim=1)\n","\n","    return 1 - torch.mm(features, others.t()).cpu().numpy()"],"metadata":{"id":"dmWdx96jwOPQ","executionInfo":{"status":"ok","timestamp":1649604454121,"user_tz":-120,"elapsed":36,"user":{"displayName":"Marah Gamdou","userId":"12048726292860942069"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["def nearest_neighbors(graph, node):\n","    return np.array(list(map(itemgetter(1),\n","                    sorted([(e[2]['weight'], e[1])\n","                            for e in graph.edges(node, data=True)]))))"],"metadata":{"id":"ILh4Ub8JCc5b","executionInfo":{"status":"ok","timestamp":1649605638460,"user_tz":-120,"elapsed":360,"user":{"displayName":"Marah Gamdou","userId":"12048726292860942069"}}},"execution_count":20,"outputs":[]},{"cell_type":"code","source":["def queries_in_graph(graph, queries, N):\n","  indices = np.zeros((len(queries), N), dtype='int')\n","  for i in range(len(queries)):\n","    nn = nearest_neighbors(graph, queries[i])\n","    indices[i,:len(nn)] = nn\n","    indices[i,len(nn):] = list(set(range(N)) - set(indices[i,:len(nn)]))\n","    \n","  return indices"],"metadata":{"id":"TZtNXOf0Cdjk","executionInfo":{"status":"ok","timestamp":1649605638934,"user_tz":-120,"elapsed":4,"user":{"displayName":"Marah Gamdou","userId":"12048726292860942069"}}},"execution_count":21,"outputs":[]},{"cell_type":"markdown","source":["# Data"],"metadata":{"id":"emJmtPOpzp_G"}},{"cell_type":"code","execution_count":51,"metadata":{"id":"BfsOL2qR9ihK","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1649608598130,"user_tz":-120,"elapsed":3403,"user":{"displayName":"Marah Gamdou","userId":"12048726292860942069"}},"outputId":"ef0e6be3-a2bc-4160-ca1c-adaad7d80faa"},"outputs":[{"output_type":"stream","name":"stdout","text":["number of unique ids:  751\n","Using a subsample of 100 ids\n","selected 5146\n","Graph(num_nodes=5146, num_edges=51460,\n","      ndata_schemes={'feat': Scheme(shape=(2048,), dtype=torch.float32), 'label': Scheme(shape=(), dtype=torch.int64)}\n","      edata_schemes={'feat': Scheme(shape=(1,), dtype=torch.float32), 'label': Scheme(shape=(), dtype=torch.int64), 'train_mask': Scheme(shape=(), dtype=torch.bool)})\n"]}],"source":["class MarketDataset(DGLDataset):\n","    def __init__(self):\n","        super().__init__(name='mydataset')\n","\n","    def process(self):\n","        dataset = 'Market'\n","\n","        with open(f'datasets/{dataset}/feat.pkl', 'rb') as f :\n","            features = pickle.load(f)  # (19281, 2048) [:100,:]\n","        with open(f'datasets/{dataset}/ids.pkl', 'rb') as f :\n","            pids = pickle.load(f) # 751 unique id [:100]\n","        with open(f'datasets/{dataset}/camids.pkl', 'rb') as f :\n","            camids = pickle.load(f)  # 6 unique camids [:100]\n","        print('number of unique ids: ', len(np.unique(pids)))\n","        \n","        number_ids = 100\n","        k=10\n","        selected_ids = np.unique(pids)[:number_ids]\n","        print(f'Using a subsample of {number_ids} ids')\n","        def select(id):\n","            return id in selected_ids\n","\n","        self.selector = list(map(select, pids))\n","        print('selected', sum(self.selector))\n","        self.features = features[self.selector]\n","        self.pids = pids[self.selector]\n","        self.camids = camids[self.selector]\n","\n","        features = torch.from_numpy(self.features)\n","        pids = torch.from_numpy(self.pids)\n","        camids = torch.from_numpy(self.camids)\n","\n","        self.original_score = compute_cosine_similarity(features, features) \n","        S, initial_rank = self.original_score.topk(k=k, dim=-1, largest=True, sorted=True)\n","        src = torch.cat([ torch.tensor(list(range(len(pids)))) for _ in range(k)])\n","        dst = torch.cat([initial_rank[:,i] for i in range(k)])\n","        scr = torch.cat([S[:,i] for i in range(k)])\n","\n","        labels = torch.zeros_like(src)\n","        n=0\n","        for s,d in zip(src, dst):\n","            if pids[s] == pids[d]:\n","                labels[n] = 1\n","            n += 1\n","        \n","        self.graph = dgl.graph( (src, dst), num_nodes=pids.shape[0])\n","        self.graph.ndata['feat'] = features\n","        self.graph.ndata['label'] = pids\n","        #self.graph.edata['weight'] = edge_features\n","        self.graph.edata['feat'] = scr[:,None]\n","        self.graph.edata['label'] = labels#[:,None]\n","        self.graph.edata['train_mask'] = torch.zeros(len(labels), dtype=torch.bool).bernoulli(0.7)\n","\n","    def __getitem__(self, i):\n","        return self.graph\n","\n","    def __len__(self):\n","        return 1\n","\n","dataset = MarketDataset()\n","graph = dataset[0]\n","\n","print(graph)"]},{"cell_type":"code","source":["original_score = compute_cosine_similarity(torch.from_numpy(dataset.features), torch.from_numpy(dataset.features))"],"metadata":{"id":"jZ1napDvZhkT","executionInfo":{"status":"ok","timestamp":1649609873910,"user_tz":-120,"elapsed":2052,"user":{"displayName":"Marah Gamdou","userId":"12048726292860942069"}}},"execution_count":94,"outputs":[]},{"cell_type":"code","source":["graph = nx.DiGraph()\n","src = list(dataset.graph.edges()[0])\n","dst = list(dataset.graph.edges()[1])\n","graph.add_nodes_from(range(5146))\n","\n","edges = []\n","for i, e in enumerate(zip(src,dst)):\n","  edges.append((int(e[0]), int(e[1]), -original_score[int(e[0]),int(e[1])]))\n","\n","graph.add_weighted_edges_from(edges) "],"metadata":{"id":"pYD0gAnibF53","executionInfo":{"status":"ok","timestamp":1649610102281,"user_tz":-120,"elapsed":3951,"user":{"displayName":"Marah Gamdou","userId":"12048726292860942069"}}},"execution_count":98,"outputs":[]},{"cell_type":"code","source":["queries = list(graph.nodes())[:300]\n","indices = queries_in_graph(graph, queries,len(graph.nodes()))"],"metadata":{"id":"S8fSTUW-bI-x","executionInfo":{"status":"ok","timestamp":1649610107933,"user_tz":-120,"elapsed":359,"user":{"displayName":"Marah Gamdou","userId":"12048726292860942069"}}},"execution_count":99,"outputs":[]},{"cell_type":"code","source":["all_cmc, all_AP, all_INP = eval_metrics('Market', indices, q_ids=dataset.pids[:300], g_ids=dataset.pids, q_camids=dataset.camids[:300], g_camids=dataset.camids, max_rank=10)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NT7MGtZIbJLW","executionInfo":{"status":"ok","timestamp":1649610116449,"user_tz":-120,"elapsed":4483,"user":{"displayName":"Marah Gamdou","userId":"12048726292860942069"}},"outputId":"e76c5a4f-8887-4fd9-bb88-7c31dde9c63a"},"execution_count":100,"outputs":[{"output_type":"stream","name":"stdout","text":["number of queries that do not exist in the gallery : 0\n","Evaluation results in csv format: \n","\u001b[36m| Dataset   | Rank-1   | Rank-5   | Rank-10   | mAP   | mINP   | metric   |\n","|:----------|:---------|:---------|:----------|:------|:-------|:---------|\n","| Market    | 85.33    | 94.33    | 95.67     | 34.58 | 7.84   | 59.96    |\u001b[0m\n"]}]},{"cell_type":"markdown","source":["# GNN"],"metadata":{"id":"BiKdGKQXzvAw"}},{"cell_type":"code","source":["class SAGE(nn.Module):\n","    def __init__(self, in_feats, hid_feats, out_feats):\n","        super().__init__()\n","        self.conv1 = dglnn.SAGEConv(\n","            in_feats=in_feats, out_feats=hid_feats, aggregator_type='mean')\n","        self.conv2 = dglnn.SAGEConv(\n","            in_feats=hid_feats, out_feats=out_feats, aggregator_type='mean')\n","\n","    def forward(self, graph, inputs):\n","        # inputs are features of nodes\n","        h = self.conv1(graph, inputs)\n","        h = F.relu(h)\n","        h = self.conv2(graph, h)\n","        return h\n","    \n","class DotProductPredictor(nn.Module):\n","    def forward(self, graph, h):\n","        # h contains the node representations computed from the GNN defined\n","        # in the node classification section (Section 5.1).\n","        with graph.local_scope():\n","            graph.ndata['h'] = h\n","            graph.apply_edges(fn.u_dot_v('h', 'h', 'score'))\n","            return graph.edata['score']\n","\n","class MLPPredictor(nn.Module):\n","    def __init__(self, in_features, out_classes):\n","        super().__init__()\n","        self.W = nn.Linear(in_features * 2, out_classes)\n","\n","    def apply_edges(self, edges):\n","        h_u = edges.src['h']\n","        h_v = edges.dst['h']\n","        score = self.W(torch.cat([h_u, h_v], 1))\n","        return {'score': score}\n","\n","    def forward(self, graph, h):\n","        # h contains the node representations computed from the GNN defined\n","        # in the node classification section (Section 5.1).\n","        with graph.local_scope():\n","            graph.ndata['h'] = h\n","            graph.apply_edges(self.apply_edges)\n","            return graph.edata['score']\n","        \n","class Model(nn.Module):\n","    def __init__(self, in_features, hidden_features, out_features):\n","        super().__init__()\n","        self.sage = SAGE(in_features, hidden_features, out_features)\n","        self.pred = MLPPredictor(out_features, 2) # DotProductPredictor()\n","    def forward(self, g, x):\n","        h = self.sage(g, x)\n","        return self.pred(g, h)\n","\n","dataset =  MarketDataset()\n","sco = DotProductPredictor()\n","graph = dataset[0]\n","print(sco(graph, graph.ndata['feat']).shape)\n","\n","node_features = graph.ndata['feat']\n","edge_label = graph.edata['label']\n","train_mask = graph.edata['train_mask']\n","model = Model(2048, 128, 64)\n","opt = torch.optim.Adam(model.parameters())\n","criterion = nn.CrossEntropyLoss()\n","for epoch in range(300):\n","    pred = model(graph, node_features)\n","    #print(pred[train_mask].shape, edge_label[train_mask].shape)\n","    loss = criterion(pred[train_mask], edge_label[train_mask])\n","    opt.zero_grad()\n","    loss.backward()\n","    opt.step()\n","    print('Epoch: ',epoch,' loss: ', loss.item(),' criterion :', criterion(pred[~train_mask], edge_label[~train_mask]).item() )"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6k_rMZnaYyyI","executionInfo":{"status":"ok","timestamp":1649608930506,"user_tz":-120,"elapsed":97813,"user":{"displayName":"Marah Gamdou","userId":"12048726292860942069"}},"outputId":"af87309b-2b38-4569-ef9f-9c8b1982eb92"},"execution_count":52,"outputs":[{"output_type":"stream","name":"stdout","text":["number of unique ids:  751\n","Using a subsample of 100 ids\n","selected 5146\n","torch.Size([51460, 1])\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/autocast_mode.py:141: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n","  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n"]},{"output_type":"stream","name":"stdout","text":["Epoch:  0  loss:  0.8615552186965942  criterion : 0.8615074753761292\n","Epoch:  1  loss:  0.2987363040447235  criterion : 0.29802241921424866\n","Epoch:  2  loss:  0.3059459626674652  criterion : 0.30457252264022827\n","Epoch:  3  loss:  0.34599733352661133  criterion : 0.3443947434425354\n","Epoch:  4  loss:  0.35275641083717346  criterion : 0.3511804938316345\n","Epoch:  5  loss:  0.34131723642349243  criterion : 0.33983471989631653\n","Epoch:  6  loss:  0.3222794830799103  criterion : 0.32082146406173706\n","Epoch:  7  loss:  0.30266305804252625  criterion : 0.30125394463539124\n","Epoch:  8  loss:  0.28665629029273987  criterion : 0.28534674644470215\n","Epoch:  9  loss:  0.27674710750579834  criterion : 0.2755874693393707\n","Epoch:  10  loss:  0.2747274339199066  criterion : 0.273699015378952\n","Epoch:  11  loss:  0.2798071801662445  criterion : 0.27888962626457214\n","Epoch:  12  loss:  0.2867882251739502  criterion : 0.2859501838684082\n","Epoch:  13  loss:  0.290097177028656  criterion : 0.2892961800098419\n","Epoch:  14  loss:  0.2884124517440796  criterion : 0.28761613368988037\n","Epoch:  15  loss:  0.2836369574069977  criterion : 0.2828209102153778\n","Epoch:  16  loss:  0.27837130427360535  criterion : 0.2775159180164337\n","Epoch:  17  loss:  0.2745373547077179  criterion : 0.2736304700374603\n","Epoch:  18  loss:  0.27287930250167847  criterion : 0.27191874384880066\n","Epoch:  19  loss:  0.27308717370033264  criterion : 0.27207574248313904\n","Epoch:  20  loss:  0.27426841855049133  criterion : 0.2732064127922058\n","Epoch:  21  loss:  0.2755601704120636  criterion : 0.2744661569595337\n","Epoch:  22  loss:  0.27626192569732666  criterion : 0.27515891194343567\n","Epoch:  23  loss:  0.2760143280029297  criterion : 0.2749171555042267\n","Epoch:  24  loss:  0.27490106225013733  criterion : 0.2738247513771057\n","Epoch:  25  loss:  0.27330920100212097  criterion : 0.27226606011390686\n","Epoch:  26  loss:  0.27176734805107117  criterion : 0.2707645297050476\n","Epoch:  27  loss:  0.2707637846469879  criterion : 0.2698045074939728\n","Epoch:  28  loss:  0.27055150270462036  criterion : 0.2696353793144226\n","Epoch:  29  loss:  0.2710039019584656  criterion : 0.2701246738433838\n","Epoch:  30  loss:  0.2716297209262848  criterion : 0.270778089761734\n","Epoch:  31  loss:  0.2718539834022522  criterion : 0.27101850509643555\n","Epoch:  32  loss:  0.27140986919403076  criterion : 0.2705792188644409\n","Epoch:  33  loss:  0.27048078179359436  criterion : 0.26964572072029114\n","Epoch:  34  loss:  0.26949676871299744  criterion : 0.2686507999897003\n","Epoch:  35  loss:  0.26882457733154297  criterion : 0.2679644525051117\n","Epoch:  36  loss:  0.2685767412185669  criterion : 0.26770323514938354\n","Epoch:  37  loss:  0.26861631870269775  criterion : 0.26773327589035034\n","Epoch:  38  loss:  0.2686886787414551  criterion : 0.2678028345108032\n","Epoch:  39  loss:  0.2685740888118744  criterion : 0.26769351959228516\n","Epoch:  40  loss:  0.2681814730167389  criterion : 0.2673141062259674\n","Epoch:  41  loss:  0.2675701081752777  criterion : 0.2667232155799866\n","Epoch:  42  loss:  0.26690149307250977  criterion : 0.26607978343963623\n","Epoch:  43  loss:  0.26634958386421204  criterion : 0.2655557096004486\n","Epoch:  44  loss:  0.2660035789012909  criterion : 0.26523706316947937\n","Epoch:  45  loss:  0.2658134400844574  criterion : 0.26507171988487244\n","Epoch:  46  loss:  0.26562026143074036  criterion : 0.26489847898483276\n","Epoch:  47  loss:  0.2652682960033417  criterion : 0.26456084847450256\n","Epoch:  48  loss:  0.2647209167480469  criterion : 0.26402243971824646\n","Epoch:  49  loss:  0.2640783488750458  criterion : 0.2633845806121826\n","Epoch:  50  loss:  0.2634843587875366  criterion : 0.2627926468849182\n","Epoch:  51  loss:  0.26300936937332153  criterion : 0.2623201906681061\n","Epoch:  52  loss:  0.26260945200920105  criterion : 0.2619256377220154\n","Epoch:  53  loss:  0.2621780037879944  criterion : 0.26150450110435486\n","Epoch:  54  loss:  0.2616308629512787  criterion : 0.26097366213798523\n","Epoch:  55  loss:  0.26096585392951965  criterion : 0.260330468416214\n","Epoch:  56  loss:  0.260258287191391  criterion : 0.25964879989624023\n","Epoch:  57  loss:  0.2595970034599304  criterion : 0.25901514291763306\n","Epoch:  58  loss:  0.25900399684906006  criterion : 0.2584490180015564\n","Epoch:  59  loss:  0.2584091126918793  criterion : 0.25787851214408875\n","Epoch:  60  loss:  0.2577199339866638  criterion : 0.2572101950645447\n","Epoch:  61  loss:  0.2569238543510437  criterion : 0.25643202662467957\n","Epoch:  62  loss:  0.2560998499393463  criterion : 0.2556242346763611\n","Epoch:  63  loss:  0.2553205192089081  criterion : 0.2548618018627167\n","Epoch:  64  loss:  0.2545686662197113  criterion : 0.2541298270225525\n","Epoch:  65  loss:  0.25376734137535095  criterion : 0.2533530294895172\n","Epoch:  66  loss:  0.25288015604019165  criterion : 0.25249508023262024\n","Epoch:  67  loss:  0.25195783376693726  criterion : 0.2516053318977356\n","Epoch:  68  loss:  0.25107041001319885  criterion : 0.2507515847682953\n","Epoch:  69  loss:  0.2502059042453766  criterion : 0.2499198317527771\n","Epoch:  70  loss:  0.24928753077983856  criterion : 0.24903258681297302\n","Epoch:  71  loss:  0.2483033984899521  criterion : 0.24807852506637573\n","Epoch:  72  loss:  0.24732933938503265  criterion : 0.2471347451210022\n","Epoch:  73  loss:  0.24639591574668884  criterion : 0.2462337464094162\n","Epoch:  74  loss:  0.24544261395931244  criterion : 0.24531613290309906\n","Epoch:  75  loss:  0.24444405734539032  criterion : 0.2443559765815735\n","Epoch:  76  loss:  0.24346385896205902  criterion : 0.24341540038585663\n","Epoch:  77  loss:  0.24252773821353912  criterion : 0.24251876771450043\n","Epoch:  78  loss:  0.2415718138217926  criterion : 0.24160230159759521\n","Epoch:  79  loss:  0.24059580266475677  criterion : 0.24066674709320068\n","Epoch:  80  loss:  0.23966720700263977  criterion : 0.23978029191493988\n","Epoch:  81  loss:  0.23876291513442993  criterion : 0.2389191836118698\n","Epoch:  82  loss:  0.23784266412258148  criterion : 0.23804199695587158\n","Epoch:  83  loss:  0.23695968091487885  criterion : 0.23720131814479828\n","Epoch:  84  loss:  0.23611724376678467  criterion : 0.23640167713165283\n","Epoch:  85  loss:  0.2352643609046936  criterion : 0.23559430241584778\n","Epoch:  86  loss:  0.23444633185863495  criterion : 0.23482383787631989\n","Epoch:  87  loss:  0.23366717994213104  criterion : 0.2340913861989975\n","Epoch:  88  loss:  0.23288625478744507  criterion : 0.2333538681268692\n","Epoch:  89  loss:  0.23214580118656158  criterion : 0.23265457153320312\n","Epoch:  90  loss:  0.2314281016588211  criterion : 0.23198015987873077\n","Epoch:  91  loss:  0.2307133972644806  criterion : 0.23131288588047028\n","Epoch:  92  loss:  0.23003873229026794  criterion : 0.23068535327911377\n","Epoch:  93  loss:  0.22936797142028809  criterion : 0.23005637526512146\n","Epoch:  94  loss:  0.22871804237365723  criterion : 0.2294439822435379\n","Epoch:  95  loss:  0.22808870673179626  criterion : 0.2288542538881302\n","Epoch:  96  loss:  0.22745996713638306  criterion : 0.2282698005437851\n","Epoch:  97  loss:  0.226857990026474  criterion : 0.22771058976650238\n","Epoch:  98  loss:  0.22625549137592316  criterion : 0.22714382410049438\n","Epoch:  99  loss:  0.22567279636859894  criterion : 0.22659307718276978\n","Epoch:  100  loss:  0.2250954508781433  criterion : 0.22605156898498535\n","Epoch:  101  loss:  0.22452832758426666  criterion : 0.2255231887102127\n","Epoch:  102  loss:  0.22397369146347046  criterion : 0.22500185668468475\n","Epoch:  103  loss:  0.22342438995838165  criterion : 0.22447893023490906\n","Epoch:  104  loss:  0.22289054095745087  criterion : 0.22397199273109436\n","Epoch:  105  loss:  0.22236000001430511  criterion : 0.22347263991832733\n","Epoch:  106  loss:  0.22184564173221588  criterion : 0.22298650443553925\n","Epoch:  107  loss:  0.22133536636829376  criterion : 0.22249718010425568\n","Epoch:  108  loss:  0.22084078192710876  criterion : 0.22202201187610626\n","Epoch:  109  loss:  0.22035065293312073  criterion : 0.22155563533306122\n","Epoch:  110  loss:  0.2198745608329773  criterion : 0.22110196948051453\n","Epoch:  111  loss:  0.21940355002880096  criterion : 0.22064679861068726\n","Epoch:  112  loss:  0.2189449816942215  criterion : 0.22020235657691956\n","Epoch:  113  loss:  0.21849091351032257  criterion : 0.219766303896904\n","Epoch:  114  loss:  0.21804727613925934  criterion : 0.21933983266353607\n","Epoch:  115  loss:  0.21760748326778412  criterion : 0.21891140937805176\n","Epoch:  116  loss:  0.21717582643032074  criterion : 0.218490332365036\n","Epoch:  117  loss:  0.21674659848213196  criterion : 0.21807563304901123\n","Epoch:  118  loss:  0.21632371842861176  criterion : 0.21766604483127594\n","Epoch:  119  loss:  0.21590189635753632  criterion : 0.2172529399394989\n","Epoch:  120  loss:  0.21548376977443695  criterion : 0.21684423089027405\n","Epoch:  121  loss:  0.215065598487854  criterion : 0.21643909811973572\n","Epoch:  122  loss:  0.2146487832069397  criterion : 0.21603353321552277\n","Epoch:  123  loss:  0.21423116326332092  criterion : 0.21562373638153076\n","Epoch:  124  loss:  0.21381287276744843  criterion : 0.21521568298339844\n","Epoch:  125  loss:  0.21339328587055206  criterion : 0.2148088663816452\n","Epoch:  126  loss:  0.21297138929367065  criterion : 0.21439683437347412\n","Epoch:  127  loss:  0.21254776418209076  criterion : 0.21398182213306427\n","Epoch:  128  loss:  0.2121206820011139  criterion : 0.2135673314332962\n","Epoch:  129  loss:  0.2116912603378296  criterion : 0.21315093338489532\n","Epoch:  130  loss:  0.21125777065753937  criterion : 0.21272754669189453\n","Epoch:  131  loss:  0.2108212560415268  criterion : 0.21230308711528778\n","Epoch:  132  loss:  0.21038095653057098  criterion : 0.21187825500965118\n","Epoch:  133  loss:  0.2099366933107376  criterion : 0.21144725382328033\n","Epoch:  134  loss:  0.20948883891105652  criterion : 0.21101178228855133\n","Epoch:  135  loss:  0.20903652906417847  criterion : 0.2105761170387268\n","Epoch:  136  loss:  0.20858044922351837  criterion : 0.2101367563009262\n","Epoch:  137  loss:  0.20811988413333893  criterion : 0.20969034731388092\n","Epoch:  138  loss:  0.20765504240989685  criterion : 0.2092430293560028\n","Epoch:  139  loss:  0.2071862667798996  criterion : 0.20879359543323517\n","Epoch:  140  loss:  0.2067129760980606  criterion : 0.2083367109298706\n","Epoch:  141  loss:  0.20623551309108734  criterion : 0.20787733793258667\n","Epoch:  142  loss:  0.20575354993343353  criterion : 0.20741643011569977\n","Epoch:  143  loss:  0.20526711642742157  criterion : 0.20694869756698608\n","Epoch:  144  loss:  0.20477637648582458  criterion : 0.20647718012332916\n","Epoch:  145  loss:  0.20428113639354706  criterion : 0.20600450038909912\n","Epoch:  146  loss:  0.20378144085407257  criterion : 0.20552536845207214\n","Epoch:  147  loss:  0.20327739417552948  criterion : 0.20504191517829895\n","Epoch:  148  loss:  0.20276889204978943  criterion : 0.20455756783485413\n","Epoch:  149  loss:  0.2022560089826584  criterion : 0.20406702160835266\n","Epoch:  150  loss:  0.20173895359039307  criterion : 0.20357243716716766\n","Epoch:  151  loss:  0.20121778547763824  criterion : 0.20307722687721252\n","Epoch:  152  loss:  0.20069272816181183  criterion : 0.20257622003555298\n","Epoch:  153  loss:  0.20016410946846008  criterion : 0.20207232236862183\n","Epoch:  154  loss:  0.1996319741010666  criterion : 0.20156803727149963\n","Epoch:  155  loss:  0.1990966796875  criterion : 0.20105868577957153\n","Epoch:  156  loss:  0.19855846464633942  criterion : 0.20054803788661957\n","Epoch:  157  loss:  0.19801756739616394  criterion : 0.20003747940063477\n","Epoch:  158  loss:  0.19747427105903625  criterion : 0.19952258467674255\n","Epoch:  159  loss:  0.19692903757095337  criterion : 0.1990089863538742\n","Epoch:  160  loss:  0.1963818371295929  criterion : 0.19849470257759094\n","Epoch:  161  loss:  0.1958327740430832  criterion : 0.19797764718532562\n","Epoch:  162  loss:  0.19528254866600037  criterion : 0.1974634975194931\n","Epoch:  163  loss:  0.1947314590215683  criterion : 0.19694729149341583\n","Epoch:  164  loss:  0.19417937099933624  criterion : 0.19643127918243408\n","Epoch:  165  loss:  0.1936262547969818  criterion : 0.19591760635375977\n","Epoch:  166  loss:  0.19307243824005127  criterion : 0.19540108740329742\n","Epoch:  167  loss:  0.19251783192157745  criterion : 0.19488807022571564\n","Epoch:  168  loss:  0.1919616460800171  criterion : 0.1943712681531906\n","Epoch:  169  loss:  0.19140315055847168  criterion : 0.19385218620300293\n","Epoch:  170  loss:  0.19084271788597107  criterion : 0.19333408772945404\n","Epoch:  171  loss:  0.19028523564338684  criterion : 0.19281643629074097\n","Epoch:  172  loss:  0.18973086774349213  criterion : 0.19229920208454132\n","Epoch:  173  loss:  0.18918544054031372  criterion : 0.191786989569664\n","Epoch:  174  loss:  0.18864208459854126  criterion : 0.19128166139125824\n","Epoch:  175  loss:  0.18810589611530304  criterion : 0.19079114496707916\n","Epoch:  176  loss:  0.18758194148540497  criterion : 0.1903132051229477\n","Epoch:  177  loss:  0.18706151843070984  criterion : 0.18983839452266693\n","Epoch:  178  loss:  0.18653777241706848  criterion : 0.18936443328857422\n","Epoch:  179  loss:  0.1860095113515854  criterion : 0.18888670206069946\n","Epoch:  180  loss:  0.18547986447811127  criterion : 0.18841038644313812\n","Epoch:  181  loss:  0.18495403230190277  criterion : 0.18794022500514984\n","Epoch:  182  loss:  0.18443773686885834  criterion : 0.1874815821647644\n","Epoch:  183  loss:  0.1839303821325302  criterion : 0.18703344464302063\n","Epoch:  184  loss:  0.18343181908130646  criterion : 0.18658976256847382\n","Epoch:  185  loss:  0.1829344928264618  criterion : 0.18615488708019257\n","Epoch:  186  loss:  0.18243706226348877  criterion : 0.18571147322654724\n","Epoch:  187  loss:  0.18193970620632172  criterion : 0.18528513610363007\n","Epoch:  188  loss:  0.18144860863685608  criterion : 0.184834286570549\n","Epoch:  189  loss:  0.18096664547920227  criterion : 0.18444626033306122\n","Epoch:  190  loss:  0.180516317486763  criterion : 0.18398858606815338\n","Epoch:  191  loss:  0.18016010522842407  criterion : 0.18382708728313446\n","Epoch:  192  loss:  0.18009915947914124  criterion : 0.18356624245643616\n","Epoch:  193  loss:  0.18084923923015594  criterion : 0.18483732640743256\n","Epoch:  194  loss:  0.18092642724514008  criterion : 0.18431463837623596\n","Epoch:  195  loss:  0.18004658818244934  criterion : 0.18415842950344086\n","Epoch:  196  loss:  0.17792588472366333  criterion : 0.1817641705274582\n","Epoch:  197  loss:  0.1787475049495697  criterion : 0.1823909431695938\n","Epoch:  198  loss:  0.17948362231254578  criterion : 0.1838059276342392\n","Epoch:  199  loss:  0.17696627974510193  criterion : 0.180898979306221\n","Epoch:  200  loss:  0.1775871366262436  criterion : 0.18138229846954346\n","Epoch:  201  loss:  0.17828397452831268  criterion : 0.182748943567276\n","Epoch:  202  loss:  0.17587026953697205  criterion : 0.1799864023923874\n","Epoch:  203  loss:  0.1769891232252121  criterion : 0.18086889386177063\n","Epoch:  204  loss:  0.17704999446868896  criterion : 0.18163709342479706\n","Epoch:  205  loss:  0.17492976784706116  criterion : 0.17927342653274536\n","Epoch:  206  loss:  0.17663437128067017  criterion : 0.1805819869041443\n","Epoch:  207  loss:  0.1756458729505539  criterion : 0.1803339272737503\n","Epoch:  208  loss:  0.1743140071630478  criterion : 0.17890170216560364\n","Epoch:  209  loss:  0.17587319016456604  criterion : 0.17994050681591034\n","Epoch:  210  loss:  0.1739782989025116  criterion : 0.17871098220348358\n","Epoch:  211  loss:  0.17385347187519073  criterion : 0.17865964770317078\n","Epoch:  212  loss:  0.17424581944942474  criterion : 0.17853288352489471\n","Epoch:  213  loss:  0.17259764671325684  criterion : 0.1773078441619873\n","Epoch:  214  loss:  0.17318615317344666  criterion : 0.1781574785709381\n","Epoch:  215  loss:  0.17240850627422333  criterion : 0.1769985407590866\n","Epoch:  216  loss:  0.1719338297843933  criterion : 0.1766258180141449\n","Epoch:  217  loss:  0.1721731424331665  criterion : 0.1772635579109192\n","Epoch:  218  loss:  0.17120373249053955  criterion : 0.17609627544879913\n","Epoch:  219  loss:  0.1714656800031662  criterion : 0.17621341347694397\n","Epoch:  220  loss:  0.17101211845874786  criterion : 0.17618948221206665\n","Epoch:  221  loss:  0.17047512531280518  criterion : 0.17562155425548553\n","Epoch:  222  loss:  0.17067603766918182  criterion : 0.17556613683700562\n","Epoch:  223  loss:  0.16993831098079681  criterion : 0.17518767714500427\n","Epoch:  224  loss:  0.16979347169399261  criterion : 0.17513930797576904\n","Epoch:  225  loss:  0.16965053975582123  criterion : 0.17474207282066345\n","Epoch:  226  loss:  0.1690434068441391  criterion : 0.1743684709072113\n","Epoch:  227  loss:  0.16902093589305878  criterion : 0.17452993988990784\n","Epoch:  228  loss:  0.1686464101076126  criterion : 0.17395411431789398\n","Epoch:  229  loss:  0.16827161610126495  criterion : 0.1736922711133957\n","Epoch:  230  loss:  0.16819055378437042  criterion : 0.1738438755273819\n","Epoch:  231  loss:  0.16775229573249817  criterion : 0.17326809465885162\n","Epoch:  232  loss:  0.16752703487873077  criterion : 0.17306873202323914\n","Epoch:  233  loss:  0.16734495759010315  criterion : 0.17313739657402039\n","Epoch:  234  loss:  0.16693170368671417  criterion : 0.17264319956302643\n","Epoch:  235  loss:  0.16676320135593414  criterion : 0.17244938015937805\n","Epoch:  236  loss:  0.16651321947574615  criterion : 0.17244820296764374\n","Epoch:  237  loss:  0.16614043712615967  criterion : 0.17203062772750854\n","Epoch:  238  loss:  0.16597293317317963  criterion : 0.17181792855262756\n","Epoch:  239  loss:  0.1656932830810547  criterion : 0.17177234590053558\n","Epoch:  240  loss:  0.1653527021408081  criterion : 0.1714087426662445\n","Epoch:  241  loss:  0.16516943275928497  criterion : 0.17118202149868011\n","Epoch:  242  loss:  0.1648886501789093  criterion : 0.17111699283123016\n","Epoch:  243  loss:  0.16456089913845062  criterion : 0.17077064514160156\n","Epoch:  244  loss:  0.1643560379743576  criterion : 0.17053182423114777\n","Epoch:  245  loss:  0.16407878696918488  criterion : 0.17046232521533966\n","Epoch:  246  loss:  0.16374985873699188  criterion : 0.17010736465454102\n","Epoch:  247  loss:  0.16351528465747833  criterion : 0.16985753178596497\n","Epoch:  248  loss:  0.1632431149482727  criterion : 0.16978757083415985\n","Epoch:  249  loss:  0.162913516163826  criterion : 0.1694280207157135\n","Epoch:  250  loss:  0.1626676321029663  criterion : 0.1691988855600357\n","Epoch:  251  loss:  0.16243410110473633  criterion : 0.16915276646614075\n","Epoch:  252  loss:  0.16215042769908905  criterion : 0.16882289946079254\n","Epoch:  253  loss:  0.16191302239894867  criterion : 0.1686485856771469\n","Epoch:  254  loss:  0.16169923543930054  criterion : 0.1685865968465805\n","Epoch:  255  loss:  0.1614225059747696  criterion : 0.16824473440647125\n","Epoch:  256  loss:  0.1611303985118866  criterion : 0.16804824769496918\n","Epoch:  257  loss:  0.16087333858013153  criterion : 0.16788217425346375\n","Epoch:  258  loss:  0.16061639785766602  criterion : 0.16756853461265564\n","Epoch:  259  loss:  0.16034330427646637  criterion : 0.16743357479572296\n","Epoch:  260  loss:  0.16008274257183075  criterion : 0.16720235347747803\n","Epoch:  261  loss:  0.15984660387039185  criterion : 0.16696809232234955\n","Epoch:  262  loss:  0.15960286557674408  criterion : 0.16686050593852997\n","Epoch:  263  loss:  0.1593405306339264  criterion : 0.16657397150993347\n","Epoch:  264  loss:  0.1590806543827057  criterion : 0.16638769209384918\n","Epoch:  265  loss:  0.1588314026594162  criterion : 0.1662318855524063\n","Epoch:  266  loss:  0.15857680141925812  criterion : 0.1659511923789978\n","Epoch:  267  loss:  0.15831319987773895  criterion : 0.16581550240516663\n","Epoch:  268  loss:  0.1580507755279541  criterion : 0.1655685007572174\n","Epoch:  269  loss:  0.15780185163021088  criterion : 0.16535179316997528\n","Epoch:  270  loss:  0.15755939483642578  criterion : 0.1652238517999649\n","Epoch:  271  loss:  0.15731003880500793  criterion : 0.16495119035243988\n","Epoch:  272  loss:  0.15705548226833344  criterion : 0.16481198370456696\n","Epoch:  273  loss:  0.15679875016212463  criterion : 0.1645767092704773\n","Epoch:  274  loss:  0.15654484927654266  criterion : 0.1643730252981186\n","Epoch:  275  loss:  0.1562967449426651  criterion : 0.16422897577285767\n","Epoch:  276  loss:  0.1560547798871994  criterion : 0.1639605015516281\n","Epoch:  277  loss:  0.15581052005290985  criterion : 0.16389739513397217\n","Epoch:  278  loss:  0.1555643081665039  criterion : 0.16358210146427155\n","Epoch:  279  loss:  0.15531587600708008  criterion : 0.16353890299797058\n","Epoch:  280  loss:  0.15506590902805328  criterion : 0.16321028769016266\n","Epoch:  281  loss:  0.15480756759643555  criterion : 0.16314521431922913\n","Epoch:  282  loss:  0.15454500913619995  criterion : 0.16285304725170135\n","Epoch:  283  loss:  0.15429076552391052  criterion : 0.16271904110908508\n","Epoch:  284  loss:  0.15403632819652557  criterion : 0.16251128911972046\n","Epoch:  285  loss:  0.15379227697849274  criterion : 0.16230657696723938\n","Epoch:  286  loss:  0.15355190634727478  criterion : 0.162189319729805\n","Epoch:  287  loss:  0.15331418812274933  criterion : 0.16191759705543518\n","Epoch:  288  loss:  0.15309250354766846  criterion : 0.1619073748588562\n","Epoch:  289  loss:  0.15288548171520233  criterion : 0.16153857111930847\n","Epoch:  290  loss:  0.15272662043571472  criterion : 0.1617436707019806\n","Epoch:  291  loss:  0.15261948108673096  criterion : 0.16125254333019257\n","Epoch:  292  loss:  0.15262527763843536  criterion : 0.16189560294151306\n","Epoch:  293  loss:  0.15270602703094482  criterion : 0.16125920414924622\n","Epoch:  294  loss:  0.15306980907917023  criterion : 0.1626516878604889\n","Epoch:  295  loss:  0.15302467346191406  criterion : 0.16149932146072388\n","Epoch:  296  loss:  0.1529747098684311  criterion : 0.16272009909152985\n","Epoch:  297  loss:  0.15190622210502625  criterion : 0.16059692203998566\n","Epoch:  298  loss:  0.1509503275156021  criterion : 0.16040842235088348\n","Epoch:  299  loss:  0.15045487880706787  criterion : 0.15973125398159027\n"]}]},{"cell_type":"code","source":["print(model)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MbVtdmr10V6u","executionInfo":{"status":"ok","timestamp":1649608930507,"user_tz":-120,"elapsed":47,"user":{"displayName":"Marah Gamdou","userId":"12048726292860942069"}},"outputId":"60adc43b-f4dd-4223-c3c3-50f1a2827e59"},"execution_count":53,"outputs":[{"output_type":"stream","name":"stdout","text":["Model(\n","  (sage): SAGE(\n","    (conv1): SAGEConv(\n","      (feat_drop): Dropout(p=0.0, inplace=False)\n","      (fc_self): Linear(in_features=2048, out_features=128, bias=False)\n","      (fc_neigh): Linear(in_features=2048, out_features=128, bias=False)\n","    )\n","    (conv2): SAGEConv(\n","      (feat_drop): Dropout(p=0.0, inplace=False)\n","      (fc_self): Linear(in_features=128, out_features=64, bias=False)\n","      (fc_neigh): Linear(in_features=128, out_features=64, bias=False)\n","    )\n","  )\n","  (pred): MLPPredictor(\n","    (W): Linear(in_features=128, out_features=2, bias=True)\n","  )\n",")\n"]}]},{"cell_type":"code","source":["dataset.graph"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XGNP7Yd12Fk9","executionInfo":{"status":"ok","timestamp":1649608930508,"user_tz":-120,"elapsed":33,"user":{"displayName":"Marah Gamdou","userId":"12048726292860942069"}},"outputId":"65526e71-6e77-4bdb-d8a3-848f81258126"},"execution_count":54,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Graph(num_nodes=5146, num_edges=51460,\n","      ndata_schemes={'feat': Scheme(shape=(2048,), dtype=torch.float32), 'label': Scheme(shape=(), dtype=torch.int64)}\n","      edata_schemes={'feat': Scheme(shape=(1,), dtype=torch.float32), 'label': Scheme(shape=(), dtype=torch.int64), 'train_mask': Scheme(shape=(), dtype=torch.bool)})"]},"metadata":{},"execution_count":54}]},{"cell_type":"code","source":["graph = nx.DiGraph()\n","src = list(dataset.graph.edges()[0])\n","dst = list(dataset.graph.edges()[1])\n","graph.add_nodes_from(range(5146))\n","\n","edges = []\n","for i, e in enumerate(zip(src,dst)):\n","  edges.append((int(e[0]), int(e[1]), -float(prediction[i,1])))\n","\n","graph.add_weighted_edges_from(edges) "],"metadata":{"id":"Tlp2g3QDY_qg","executionInfo":{"status":"ok","timestamp":1649609562859,"user_tz":-120,"elapsed":1340,"user":{"displayName":"Marah Gamdou","userId":"12048726292860942069"}}},"execution_count":85,"outputs":[]},{"cell_type":"code","source":["queries = list(graph.nodes())[:300]\n","indices = queries_in_graph(graph, queries,len(graph.nodes()))"],"metadata":{"id":"yvn8k54jY_vb","executionInfo":{"status":"ok","timestamp":1649609569998,"user_tz":-120,"elapsed":315,"user":{"displayName":"Marah Gamdou","userId":"12048726292860942069"}}},"execution_count":86,"outputs":[]},{"cell_type":"code","source":["all_cmc, all_AP, all_INP = eval_metrics('Market', indices, q_ids=dataset.pids[:300], g_ids=dataset.pids, q_camids=dataset.camids[:300], g_camids=dataset.camids, max_rank=10)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DJvh7VzhMFMO","executionInfo":{"status":"ok","timestamp":1649609579303,"user_tz":-120,"elapsed":4560,"user":{"displayName":"Marah Gamdou","userId":"12048726292860942069"}},"outputId":"7470b745-c233-4f4e-f316-c6d7427d6638"},"execution_count":87,"outputs":[{"output_type":"stream","name":"stdout","text":["number of queries that do not exist in the gallery : 0\n","Evaluation results in csv format: \n","\u001b[36m| Dataset   | Rank-1   | Rank-5   | Rank-10   | mAP   | mINP   | metric   |\n","|:----------|:---------|:---------|:----------|:------|:-------|:---------|\n","| Market    | 75.67    | 91.33    | 95.67     | 31.65 | 6.00   | 53.66    |\u001b[0m\n"]}]}]}